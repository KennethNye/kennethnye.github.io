---
layout: single
title:  "Theory and Examples of Expectation and Maximization Algorithm"
date : 2023-02-26
header:
  teaser: "unsplash-gallery-image-2-th.jpg"
permalink: /posts/2023/02/blog-post-2/
tags:
  - problems
---

# The EM algorithm: some theory

Suppose we have some observations $x = (x_{1},x_{2},\ldots,x_{n})$ from
a mixture model, e.g., binomial mixture model, Gaussian mixture model,
negative binomial mixture model etc. Denote the parameter set of the
mixture model as $\theta$. Let's take binomial mixture model as an
example, the parameter set consists of the proportion
$\pi = (\pi_{1},\pi_{2},\ldots,\pi_{N} = 1 - \sum_{k = 1}^{N - 1}\pi_{k})$
of all $N$ binomial distributions and its corresponding success rates
$q = (q_{1},q_{2},\ldots,q_{N})$. That is to say $\theta = (\pi,q)$. We
wish to estimate the parameters $\theta$ from our observations $x$.
$l\left( \theta \middle| x \right) = \log{p\left( x \middle| \theta \right)}$
is constructed as the log-likelihood function. It is usually termed as
the incomplete log-likelihood function since we don't know for each
$x_{i}$ which portion of the mixture it came from. Our goal is
maximizing the log-likelihood function to estimate the parameter set
$\theta$. That is to say,

$$\widehat{\theta} = {argmax}_{\theta}{l\left( \theta \middle| x \right)} = {argmax}_{\theta}{\log{p\left( x \middle| \theta \right)}}$$

Now consider the complete log-likelihood function which in addition also
informs us which portion of the mixture each $x_{i}$ come from. In
another word, each $x_{i}$ is associated with a label $z_{i} = k$,
$k = 1,2,\ldots,N$.

Considering all possible labels, $l(\theta|x)$ can be written as a sum
of all possible complete log-likelihood functions.

$$l\left( \theta \middle| x \right) = \log{p\left( x \middle| \theta \right)} = \log{\sum_{z}^{}{p(x,z|\theta)}}$$

$EM$ algorithm is an iterative procedure. Suppose at $t$-th iteration,
the estimated parameter values are $\theta^{(t)}$. To improve the
accuracy of our estimates, the next iteration should result in a net
increase in $l(\theta|x)$ from its current value $l(\theta^{(t)}|x)$.
Let's look at the difference between $l(\theta|x)$ and its value
evaluated at $\theta^{(t)}$.

$$l\left( \theta \middle| x \right) - l\left( \theta^{(t)} \middle| x \right) = \log{\sum_{z}^{}{p\left( x,z \middle| \theta \right)}} - \log{p(x|\theta^{(t)})}$$

$$= \log{\sum_{z}^{}{p\left( z \middle| x,\theta^{(t)} \right)\frac{p\left( x,z \middle| \theta \right)}{p\left( z \middle| x,\theta^{(t)} \right)}}} - \log{p(x|\theta^{(t)})}$$

$$\geq \sum_{z}^{}{p\left( z \middle| x,\theta^{(t)} \right)\log\frac{p\left( x,z \middle| \theta \right)}{p\left( z \middle| x,\theta^{(t)} \right)}} - \log{p(x|\theta^{(t)})}$$

$$= \sum_{z}^{}{p\left( z \middle| x,\theta^{(t)} \right)\log{p\left( x,z \middle| \theta \right)}} - \sum_{0}^{1}{p\left( z \middle| x,\theta^{(t)} \right)\log{p\left( z \middle| x,\theta^{(t)} \right)}} - \log{p(x|\theta^{(t)})}$$

$$= \sum_{z}^{}{p\left( z \middle| x,\theta^{(t)} \right)\log{p\left( x,z \middle| \theta \right)}} - \sum_{0}^{1}{p\left( z \middle| x,\theta^{(t)} \right)\log{p\left( x,z \middle| \theta^{(t)} \right)}}$$

Since the second term is a constant, to increase $l(\theta|x)$, we can
maximize the difference. Since the second term is a constant, it is
equivalent to maximize the first term. The first term can be succinctly
written in the form of expectation. This is the $E$-step:

$$E_{p\left( z \middle| x,\theta^{(t)} \right)}\left( \log{p\left( x,z \middle| \theta \right)} \right) \equiv Q\left( \theta \middle| \theta^{(t)} \right)$$

Subsequently, in $(t + 1)$th step, the $M$-step is

$$\theta^{(t + 1)} = {argmax}_{\theta}\left( l\left( \theta \middle| x \right) - l\left( \theta^{(t)} \middle| x \right) \right)$$

$$= {argmax}_{\theta}\left( E_{p\left( z \middle| x,\theta^{(t)} \right)}\left( \log{p\left( x,z \middle| \theta \right)} \right) \right)$$

$$= {argmax}_{\theta}\left( Q\left( \theta \middle| \theta^{(t)} \right) \right)$$

Reference:

<http://www.cs.cmu.edu/~dgovinda/pdf/recog/EM_algorithm-1.pdf>

# EM algorithms examples

## Negative binomial mixture model

Let $\theta = (\pi,q)$ be the parameter of the negative binomial mixture
model and $r$ be a constant vector, where
$\pi,r,q \in \mathbb{R}_{\geq 0}^{N}$. $x,z \in \mathbb{N}^{n}$ be the
observed number of trials and the unobserved labels.

The negative binomial mixture model is,

$$Z_{j} \sim {Mult}\pi,j = 1,2,\ldots,N$$

$$X_{j}|Z_{j} \sim NB\left( r_{Z_{j}},q_{Z_{j}} \right),j = 1,2,..,n$$

We write $Z_{j} = k$ if the $k_{th}$ coordinate of $Z_{j}$ is $1$, then
the above model can be succinctly written as,

$$X_{j} \sim \sum_{k = 1}^{N}{\pi_{k}NB\left( r_{k},q_{k} \right)}$$

Therefore, the complete log-likelihood function is given as,

$$\log{p\left( x,z \middle| \theta \right)} = \sum_{i = 1}^{n}{\log{p\left( x_{i},z_{i} \middle| \theta \right)}}$$

$$= \sum_{i = 1}^{n}{\log{p\left( x_{i} \middle| z_{i},\theta \right)}p\left( z_{i} \middle| \theta \right)}$$

$$= \sum_{i = 1}^{n}{\log{\pi_{z_{i}}\left( \begin{array}{r}
x_{i} - 1 \\
r_{z_{i}} - 1 \\
\end{array} \right)q_{z_{i}}^{r_{z_{i}}}\left( 1 - q_{z_{i}} \right)^{x_{i} - r_{z_{i}}}}}$$

$$= \sum_{i = 1}^{n}{\log\pi_{z_{i}} + \log{\left( x_{i} - 1 \right)!} - \log{\left( r_{z_{i}} - 1 \right)!} - \log{\left( x_{i} - r_{z_{i}} \right)!} + r_{z_{i}}\log q_{z_{i}} + \left( x_{i} - r_{z_{i}} \right)\log\left( 1 - q_{z_{i}} \right)}$$

$E$-step:

$$Q\left( \theta \middle| \theta^{(t)} \right) = E_{p\left( z \middle| x,\theta^{(t)} \right)}\left( \log{p\left( x,z \middle| \theta \right)} \right)$$

$$= E_{p\left( z \middle| x,\theta^{(t)} \right)}\left( \sum_{i = 1}^{n}{\log\pi_{z_{i}} + \log{\left( x_{i} - 1 \right)!} - \log{\left( r_{z_{i}} - 1 \right)!}} - \log{\left( x_{i} - r_{z_{i}} \right)!} + r_{z_{i}}\log q_{z_{i}} + \left( x_{i} - r_{z_{i}} \right)\log\left( 1 - q_{z_{i}} \right) \right)$$

$$= \sum_{i = 1}^{n}{E_{p\left( z_{i} \middle| x_{i},\theta^{(t)} \right)}\left( \log\pi_{z_{i}} \right) + \log{\left( x_{i} - 1 \right)!} - E_{p\left( z_{i} \middle| x_{i},\theta^{(t)} \right)}\left( \log{\left( r_{z_{i}} - 1 \right)!} \right)} - E_{p\left( z_{i} \middle| x_{i},\theta^{(t)} \right)}\left( \log{\left( x_{i} - r_{z_{i}} \right)!} \right) + E_{p\left( z_{i} \middle| x_{i},\theta^{(t)} \right)}\left( r_{z_{i}}\log q_{z_{i}} \right) + E_{p\left( z_{i} \middle| x_{i},\theta^{(t)} \right)}\left( \left( x_{i} - r_{z_{i}} \right)\log\left( 1 - q_{z_{i}} \right) \right)$$

$$= \sum_{i = 1}^{n}{\sum_{j = 1}^{N}{\gamma_{ij}^{(t)}\log\pi_{j}} + \log{\left( x_{i} - 1 \right)!} - \gamma_{ij}^{(t)}\log{\left( r_{j} - 1 \right)!}} - \gamma_{ij}^{(t)}\log{\left( x_{i} - r_{j} \right)!} + \gamma_{ij}^{(t)}r_{j}\log q_{j} + \gamma_{ij}^{(t)}\left( x_{i} - r_{j} \right)\log\left( 1 - q_{j} \right)$$

where

$$\gamma_{ij}^{(t)} = \frac{\left( \begin{array}{r}
x_{i} - 1 \\
r_{j} - 1 \\
\end{array} \right)\left( q_{j}^{(t)} \right)^{r_{j}}\left( 1 - q_{j}^{(t)} \right)^{x_{i} - r_{j}}}{\sum_{k = 1}^{N}{\left( \begin{array}{r}
x_{i} - 1 \\
r_{k} - 1 \\
\end{array} \right)\left( q_{k}^{(t)} \right)^{r_{k}}\left( 1 - q_{k}^{(t)} \right)^{x_{i} - r_{k}}}}$$

$M$-step:

$$\frac{\partial Q\left( \theta \middle| \theta^{t} \right)}{\partial\pi_{j}} = \sum_{i = 1}^{n}{\frac{\gamma_{ij}^{(t)}}{\pi_{j}} - \frac{1 - \sum_{k = 1}^{N - 1}\gamma_{i,k}^{(t)}}{1 - \sum_{k = 1}^{N - 1}\pi_{k}}} = 0,j = 1,2,\ldots,N - 1$$

$$\pi_{j}^{(t + 1)} = \frac{1}{n}\sum_{i = 1}^{n}\gamma_{ij}^{(t)}$$

$$\frac{\partial Q\left( \theta \middle| \theta^{(t)} \right)}{\partial q_{j}} = \sum_{i = 1}^{n}{\frac{\gamma_{ij}^{(t)}r_{j}}{q_{j}} - \frac{\gamma_{ij}^{(t)}\left( x_{i} - r_{j} \right)}{1 - q_{j}}} = 0,j = 1,2,\ldots,N$$

$$q_{j}^{(t + 1)} = \frac{\sum_{i = 1}^{n}{\gamma_{ij}^{(t)}r_{j}}}{\sum_{i = 1}^{n}{\gamma_{ij}^{(t)}x_{i}}}$$

## A mixture of two binomial distributions model

You have two coins with unknown probabilities of heads, denoted $p$, and
$q$ respectively. The first coin is chosen with probability $\pi$ and
the second coin is chosen with probability $1\  - \ \pi$. The chosen
coin is flipped once, and the result is recorded.
$x\  = \ \{ 1,\ 1,\ 0,\ 1,\ 0,\ 0,\ 1,\ 0,\ 0,\ 0,\ 1,\ 1\}$ with
$(Heads\  = \ 1,\ Tails\  = \ 0)$. Let $z_{i} \in \{ 0,\ 1\}$ denote
which coin was used on each toss.

Suppose we have,

$$X_{i} \sim \pi B(1,p) + (1 - \pi)B(1,q),i = 1,2,\ldots,n$$

The incomplete log-likelihood function is,

$$\log{p\left( x \middle| \theta \right)} = \sum_{i = 1}^{n}{\log{p\left( x_{i} \middle| \theta \right)}}$$

The complete log-likelihood function is,

$$\log{p\left( x,z \middle| \theta \right)} = \sum_{i = 1}^{n}{\log{p\left( x_{i},z_{i} \middle| \theta \right)}}$$

$$= \sum_{i = 1}^{n}{\log{p\left( x_{i} \middle| z_{i},\theta \right)p(z_{i}|\theta)}}$$

$$= \sum_{i = 1}^{n}{\log{\left( \pi{p^{x_{1}}(1 - p)}^{1 - x_{i}} \right)^{z_{i}}\left( (1 - \pi){q^{x_{1}}(1 - q)}^{1 - x_{i}} \right)^{1 - z_{i}}}}$$

$$= \sum_{i = 1}^{n}{z_{i}\left( \log\pi + x_{i}\log p + \left( 1 - x_{i} \right)\log(1 - p) \right) + \left( 1 - z_{i} \right)\left( \log(1 - \pi) + x_{i}\log q + \left( 1 - x_{i} \right)\log(1 - q) \right)}$$

The $E$-step is to calculate the expectation,

$$Q\left( \theta \middle| \theta^{(t)} \right) = E_{p\left( z \middle| x,\theta^{(t)} \right)}\left( \log{p\left( x,z \middle| \theta \right)} \right)$$

$$= E_{p\left( z \middle| x,\theta^{(t)} \right)}\left( \sum_{i = 1}^{n}{z_{i}\left( \log\pi + x_{i}\log p + \left( 1 - x_{i} \right)\log(1 - p) \right) + \left( 1 - z_{i} \right)\left( \log(1 - \pi) + x_{i}\log q + \left( 1 - x_{i} \right)\log(1 - q) \right)} \right)$$

$$= \sum_{i = 1}^{n}{E_{p\left( z \middle| x,\theta^{(t)} \right)}(z_{i})\left( \log\pi + x_{i}\log p + \left( 1 - x_{i} \right)\log(1 - p) \right) + \left( 1 - {E_{p\left( z \middle| x,\theta^{(t)} \right)}(z}_{i}) \right)\left( \log(1 - \pi) + x_{i}\log q + \left( 1 - x_{i} \right)\log(1 - q) \right)}$$

$$= \sum_{i = 1}^{n}{E\left(z_{i}|x_{i},\theta^{(t)}\right)\left( \log\pi + x_{i}\log p + \left( 1 - x_{i} \right)\log(1 - p) \right) + \left( 1 - E\left(z_{i}|x_{i},\theta^{(t)}\right) \right)\left( \log(1 - \pi) + x_{i}\log q + \left( 1 - x_{i} \right)\log(1 - q) \right)}$$

where

$$E\left( z_{i}\middle| x_{i},\theta^{(t)} \right) = \frac{\pi^{(t)}{(p^{(t)})}^{x_{i}}\left( 1 - p^{(t)} \right)^{1 - x_{i}}}{\pi^{(t)}{(p^{(t)})}^{x_{i}}\left( 1 - p^{(t)} \right)^{1 - x_{i}} + \left( 1 - \pi^{(t)} \right){(q^{(t)})}^{x_{i}}\left( 1 - q^{(t)} \right)^{1 - x_{i}}}$$

The $M$-step is

$$\frac{\partial Q\left( \theta \middle| \theta^{(t)} \right)}{\partial\theta} = 0$$

That is to say,

$$\frac{\partial Q\left( \theta \middle| \theta^{(t)} \right)}{\partial\pi} = \sum_{i = 1}^{n}{\left( \frac{\mu_{i}^{(t)}}{\pi} - \frac{1 - \mu_{i}^{(t)}}{1 - \pi} \right) = 0}$$

$$\frac{\partial Q\left( \theta \middle| \theta^{(t)} \right)}{\partial p} = \sum_{i = 1}^{n}{\mu_{i}^{(t)}\left( \frac{x_{i}}{p} - \frac{1 - x_{i}}{1 - p} \right) = 0}$$

$$\frac{\partial Q\left( \theta \middle| \theta^{(t)} \right)}{\partial q} = \sum_{i = 1}^{n}{\left( 1 - \mu_{i}^{(t)} \right)\left( \frac{x_{i}}{q} - \frac{1 - x_{i}}{1 - q} \right) = 0}$$

Therefore, we have

$$\pi^{(t + 1)} = \frac{1}{n}\sum_{i = 1}^{n}\mu_{i}^{(t)}$$

$$p^{(t + 1)} = \frac{\sum_{i = 1}^{n}{x_{i}\mu_{i}^{(t)}}}{\sum_{i = 1}^{n}\mu_{i}^{(t)}}$$

$$q^{(t + 1)} = \frac{\sum_{i = 1}^{n}{x_{i}\left( 1 - \mu_{i}^{(t)} \right)}}{\sum_{i = 1}^{n}\left( 1 - \mu_{i}^{(t)} \right)}$$

## Gaussian mixture model

Suppose we have

$$X \sim \sum_{i = 1}^{N}{\pi_{i}N\left( \mu_{i},\sigma_{i} \right)}$$

The complete log-likelihood function is

$$\log{p(x,z|\theta)} = \sum_{i = 1}^{n}{\log{\frac{1}{\sqrt{2\pi}\sigma_{z_{i}}}e^{- \frac{\left( x_{i} - \mu_{z_{i}} \right)^{2}}{2\sigma_{z_{i}}^{2}}}\pi_{z_{i}}}}$$

The $E$-step

$$Q\left( \theta \middle| \theta^{(t)} \right) = E_{p\left( z \middle| x,\theta^{(t)} \right)}\left( \sum_{i = 1}^{n}{\log{\frac{1}{\sqrt{2\pi}\sigma_{z_{i}}}e^{- \frac{\left( x_{i} - \mu_{z_{i}} \right)^{2}}{2\sigma_{z_{i}}^{2}}}\pi_{z_{i}}}} \right)$$

$$= E_{p\left( z \middle| x,\theta^{(t)} \right)}\left( \sum_{i = 1}^{n}{\log\pi_{z_{i}} - \frac{1}{2}\log{2\pi} - \log\sigma_{z_{i}} -}\frac{\left( x_{i} - \mu_{z_{i}} \right)^{2}}{2\sigma_{z_{i}}^{2}} \right)$$

$$= \sum_{i = 1}^{n}{E_{p\left( z \middle| x,\theta^{(t)} \right)}\left( \log\pi_{z_{i}} - \frac{1}{2}\log{2\pi} - \log\sigma_{z_{i}} - \frac{\left( x_{i} - \mu_{z_{i}} \right)^{2}}{2\sigma_{z_{i}}^{2}} \right)}$$

$$= \sum_{i = 1}^{n}{E_{p\left( z_{i} \middle| x_{i},\theta^{(t)} \right)}\left( \log\pi_{z_{i}} - \frac{1}{2}\log{2\pi} - \log\sigma_{z_{i}} - \frac{\left( x_{i} - \mu_{z_{i}} \right)^{2}}{2\sigma_{z_{i}}^{2}} \right)}$$

$$= \sum_{i = 1}^{n}{\sum_{j = 1}^{N}\gamma_{ij}^{(t)}\left( \log\pi_{j} - \frac{1}{2}\log{2\pi} - \log\sigma_{j} - \frac{\left( x_{i} - \mu_{j} \right)^{2}}{2\sigma_{j}^{2}} \right)}$$

where,

$$\gamma_{ij}^{(t)} = \frac{\pi_{j}^{(t)}\frac{1}{\sqrt{2\pi}\sigma_{j}^{(t)}}e^{- \frac{\left( x_{i} - \mu_{j}^{(t)} \right)^{2}}{2\left( \sigma_{j}^{(t)} \right)^{2}}}}{\sum_{k = 1}^{N}{\pi_{k}^{(t)}\frac{1}{\sqrt{2\pi}\sigma_{k}^{(t)}}e^{- \frac{\left( x_{i} - \mu_{k}^{(t)} \right)^{2}}{2\left( \sigma_{k}^{(t)} \right)^{2}}}}}$$

$M$-step is

$$\frac{\partial Q\left( \theta \middle| \theta^{(t)} \right)}{\partial\theta} = 0$$

Notice that $\pi_{N} = 1 - \sum_{k = 1}^{N - 1}\pi_{k}$ and
$\gamma_{iN}^{(t)} = 1 - \sum_{k = 1}^{N - 1}\gamma_{i,k}^{(t)}$, then,

$$\frac{\partial Q\left( \theta \middle| \theta^{(t)} \right)}{\partial\pi_{j}} = \sum_{i = 1}^{n}{\frac{\gamma_{ij}^{(t)}}{\pi_{j}} - \frac{1 - \sum_{k = 1}^{N - 1}\gamma_{i,k}^{(t)}}{1 - \sum_{k = 1}^{N - 1}\pi_{k}}} = 0,j = 1,2,\ldots,N - 1$$

$$\frac{\partial Q\left( \theta \middle| \theta^{(t)} \right)}{\partial\mu_{j}} = \sum_{i = 1}^{n}{\gamma_{ij}^{(t)}\frac{\left( x_{i} - \mu_{j} \right)}{\sigma_{j}^{2}}} = 0,j = 1,2,\ldots,N$$

$$\frac{\partial Q\left( \theta \middle| \theta^{(t)} \right)}{\partial\sigma_{j}} = \sum_{i = 1}^{n}{\gamma_{ij}^{(t)}\left( - \frac{1}{\sigma_{j}} + \frac{\left( x_{i} - \mu_{j} \right)^{2}}{\sigma_{j}^{3}} \right)} = 0,j = 1,2,\ldots,N$$

Solve these equations, we have

$$\pi_{j}^{(t + 1)} = \frac{\sum_{i = 1}^{n}\gamma_{ij}^{(t)}}{n}: = \frac{n_{j}^{(t)}}{n}$$

$$\mu_{j}^{(t + 1)} = \frac{1}{n_{j}^{(t)}}\sum_{i = 1}^{n}{x_{i}\gamma_{ij}^{(t)}}$$

$$\sigma_{j}^{(t + 1)} = \sqrt{\frac{1}{n_{j}^{(t)}}\sum_{i = 1}^{n}{\left( x_{i} - \mu_{j}^{(t + 1)} \right)^{2}\gamma_{ij}^{(t)}}}$$

## Binomial mixture model

Suppose we have the following binomial mixture model,

$$X \sim \sum_{i = 1}^{N}{\pi_{i}B\left( m,q_{i} \right)}$$

The parameter set is $\theta = (\pi,q)$

The complete log-likelihood function is

$$l(\theta) = \log{p\left( x,z \middle| \theta \right)}$$

$$= \sum_{i = 1}^{n}{\log{p\left( x_{i},z_{i} \middle| \theta \right)}}$$

$$= \sum_{i = 1}^{n}{\log{p\left( x_{i} \middle| z_{i},\theta \right)}p\left( z_{i} \middle| \theta \right)}$$

$$= \sum_{i = 1}^{n}{\log{\pi_{z_{i}}\binom{m}{x_{i}}q_{z_{i}}^{x_{i}}\left( 1 - q_{z_{i}} \right)^{m - x_{1}}}}$$

$$= \sum_{i = 1}^{n}{\log\pi_{z_{i}} + \log\left( \begin{array}{r}
m \\
x_{i} \\
\end{array} \right) + x_{i}\log q_{z_{i}} + \left( m - x_{i} \right)\log\left( 1 - q_{z_{i}} \right)}$$

The E-step is

$$Q\left( \theta \middle| \theta^{(t)} \right) = E_{p\left( z \middle| x,\theta \right)}\left( \log{p\left( x,z \middle| \theta \right)} \right)$$

$$= E_{p\left( z \middle| x,\theta^{t} \right)}\left( \sum_{i = 1}^{n}{\log\pi_{z_{i}} + \log\left( \begin{array}{r}
m \\
x_{i} \\
\end{array} \right) + x_{i}\log q_{z_{i}} + \left( m - x_{i} \right)\log\left( 1 - q_{z_{i}} \right)} \right)$$

$$= \sum_{i = 1}^{n}{E_{p\left( z_{i} \middle| x_{i},\theta^{(t)} \right)}\left( \log\pi_{z_{i}} \right) + \log\left( \begin{array}{r}
m \\
x_{i} \\
\end{array} \right) + x_{i}E_{p\left( z_{i} \middle| x_{i},\theta \right)}\left( \log q_{z_{i}} \right) + \left( m - x_{i} \right)E_{p\left( z_{i} \middle| x_{i},\theta \right)}\left( \log\left( 1 - q_{z_{i}} \right) \right)}$$

where

$$\gamma_{i,z_{i}}^{(t)} ≔ p\left( z_{i} \middle| x_{i},\theta^{(t)} \right) = \frac{\pi_{z_{i}}^{(t)}\binom{m}{x_{i}}\left( q_{z_{i}}^{(t)} \right)^{x_{i}}\ \left( 1 - q_{z_{i}}^{(t)} \right)^{m - x_{1}}}{\sum_{j = 1}^{N}{\pi_{j}^{(t)}\binom{m}{x_{i}}\left( q_{j}^{(t)} \right)^{x_{i}}\left( 1 - q_{j}^{(t)} \right)^{m - x_{1}}}}$$

Therefore, we have

$$Q\left( \theta \middle| \theta^{(t)} \right)$$

$$= \sum_{i = 1}^{n}{\sum_{j = 1}^{N}\gamma_{ij}^{(t)}\log\pi_{j} + \log\left( \begin{array}{r}
m \\
x_{i} \\
\end{array} \right) + x_{i}\gamma_{ij}^{(t)}\log q_{j} + \left( m - x_{i} \right)\gamma_{ij}^{(t)}\log\left( 1 - q_{j} \right)}$$

The M-step is

$$\frac{\partial Q\left( \theta \middle| \theta^{(t)} \right)}{\partial\theta} = 0$$

That is to say,

$$\frac{\partial Q\left( \theta \middle| \theta^{(t)} \right)}{\partial\pi_{j}} = \sum_{i = 1}^{n}\left( \frac{\gamma_{ij}^{(t)}}{\pi_{j}} - \frac{1 - \sum_{k = 1}^{N - 1}\gamma_{ik}^{(t)}}{1 - \sum_{k = 1}^{N - 1}\pi_{k}} \right) = 0,j = 1,2,\ldots,N - 1\ $$

$$\pi_{j}^{(t + 1)} = \frac{1}{n}\sum_{i = 1}^{n}\gamma_{ij}^{(t)}$$

$$\frac{\partial Q\left( \theta \middle| \theta^{(t)} \right)}{\partial q_{j}} = \sum_{i = 1}^{n}\left( \frac{x_{i}\gamma_{ij}^{(t)}}{q_{j}} - \frac{\left( m - x_{i} \right)\gamma_{ij}^{(t)}}{1 - q_{j}} \right) = 0,j = 1,2,\ldots,N$$

$$q_{j}^{(t + 1)} = \frac{\sum_{i = 1}^{n}{x_{i}\gamma_{ij}^{(t)}}}{m\sum_{i = 1}^{n}\gamma_{ij}^{(t)}}$$

## Poisson mixture model

Suppose we have

$$X \sim \sum_{k = 1}^{N}{\pi_{k}Po(k\lambda)}$$

The complete log-likelihood function is

$$\log{p(x,z|\theta)} = \sum_{i = 1}^{n}{\log{p(x_{i},z_{i}|\theta)}}$$

$$= \sum_{i = 1}^{n}{\log{p(x_{i}|z_{i},\theta)p(z_{i}|\theta)}}$$

$$= \sum_{i = 1}^{n}{\log{\pi_{z_{i}}\frac{e^{- z_{i}\lambda}\left( z_{i}\lambda \right)^{x_{i}}}{x_{i}!}}}$$

$$= \sum_{i = 1}^{n}{\log\pi_{z_{i}} + x_{i}\log{z_{i}\lambda} - z_{i}\lambda - \log{x_{i}!}}$$

The $E$-step is

$$Q\left( \theta \middle| \theta^{(t)} \right) = E_{p\left( z \middle| x,\theta^{(t)} \right)}\left( p\left( x,z \middle| \theta \right) \right)$$

$$= E_{p\left( z \middle| x,\theta^{(t)} \right)}\left( \sum_{i = 1}^{n}{\log\pi_{z_{i}} + x_{i}\log{z_{i}\lambda} - z_{i}\lambda - \log{x_{i}!}} \right)$$

$$= \sum_{i = 1}^{n}{E_{p\left( z_{i} \middle| x_{i},\theta^{(t)} \right)}\left( \log\pi_{z_{i}} \right) + x_{i}E_{p\left( z_{i} \middle| x_{i},\theta^{(t)} \right)}\left( \log z_{i} \right) + x_{i}\log\lambda - \lambda E_{p\left( z_{i} \middle| x_{i},\theta^{(t)} \right)}\left( z_{i} \right) - \log{x_{i}!}}$$

$$= \sum_{i = 1}^{n}{\sum_{j = 1}^{N}{\gamma_{ij}^{(t)}\log\pi_{j}} + x_{i}\gamma_{ij}^{(t)}\log j + x_{i}\log\lambda - \lambda\gamma_{ij}^{(t)}j - \log{x_{i}!}}$$

The $M$-step is

$$\frac{\partial Q\left( \theta \middle| \theta^{(t)} \right)}{\partial\theta} = 0$$

$$\pi_{j}^{(t + 1)} = \frac{1}{n}\sum_{i = 1}^{n}\gamma_{ij}^{(t)}$$

$$\frac{\partial Q\left( \theta \middle| \theta^{(t)} \right)}{\partial\lambda} = \sum_{i = 1}^{n}{\sum_{j = 1}^{N}{\frac{x_{i}}{\lambda} - \gamma_{ij}^{(t)}j}} = 0$$

$$\lambda^{(t + 1)} = \frac{\sum_{i = 1}^{n}{\sum_{j = 1}^{N}{\gamma_{ij}^{(t)}j}}}{N\sum_{i = 1}^{n}x_{i}}$$
