---
layout: single
title:  "Point Processes and Random Measures"
date : 2023-10-05
header:
  teaser: "unsplash-gallery-image-2-th.jpg"
permalink: /posts/2023/10/blog-post-2/
tags:
  - Research
---

This article is adapted from this [**lecture note**](https://www.stat.cmu.edu/~genovese/class/iprob-S06/notes/handoutN.pdf).

## Plan

1. What's my motivation?
2. Counting and Random Measures
3. Point Processes and Examples
4. Properties

## Elaboration

**Perspective 1** A different view of the Poisson process

We think of the Poisson process as generated from the random scatter of points on $[0, \infty)$. The process $N$ then counts the points in any set. That is, for a (measurable) set $A, N(A)$ is a random variable counting the number of points in $A$. $N$ becomes a random measure.

To formalize this, consider the collections of random variables $N(A)$ for all (Borel) sets $A$, where $N(A)$ counts the number of "points" in the set $A$.

Let $\Lambda$ be a measure on $[0, \infty)$ such that $\Lambda(A)<\infty$ for every bounded set $A$. And assume that for every collection of disjoint, bounded Borel sets $A_{1}, \ldots, A_{k}$, we have

$$
\mathrm{P}\left\{N\left(A_{j}\right)=n_{j}, j=1, \ldots, k\right\}=\prod_{j=1}^{k} \frac{\left(\Lambda\left(A_{j}\right)\right)_{j}^{n}}{n_{j} !} e^{-\Lambda\left(A_{j}\right)}
$$

That is, the $N\left(A_{j}\right)$ s are independent Poisson $\left\langle\Lambda\left(A_{j}\right)\right\rangle$ random variables.

Let's see how this works. Suppose that $\Lambda(A)=\lambda \ell(A)$, where $\ell(A)$ is the Lebesgue measure ("length") of $A$ and $\lambda>0$. Then, we have that

1. $N(\left\{0\right\})=0$
2. The probability above shows that $N\left(s_{j}, t_{j}\right]$ are independent for disjoint intervals $s_{j}<t_{j}$.

   It also shows that for any bounded Borel sets $A_{1}, \ldots, A_{k}$, the joint distribution of the random variables $N\left(A_{1}+t\right), \ldots, N\left(A_{k}+t\right)$ does not depend on $t$, where $A+t$ represents $\left\{a+t: a \in A\right\}$, for all $t$ such that all of the shifted sets are contained in $[0, \infty)$.

   In particular, $N(s, t]$ has the same distribution as $N(s+h, t+h]$ for all $h \geq-s$.

   Thus, we have stationary and independent increments.

3. $\mathrm{P}\left\{N\left(0, h\right]=1\right\}=\lambda h e^{-\lambda h}=\lambda h+o(h)$.

4. $\mathrm{P}\left\{N(0, h]>1\right\}=\sum_{k=2}^{\infty} \frac{(\lambda h)^{k}}{k !} e^{-\lambda h}=o(h)$.

This gives us back the homogeneous Poisson process.

Now suppose that $\lambda$ is a non-negative function, and define $\Lambda(A)=\int_{A} \lambda(x) d x$. Then, we have

1. $N(\left\{0\right\})=0$
2. Independent increments from the probability above.
3. $\mathrm{P}\left\{N(t, t+h]=1\right\}=\int_{t}^{t+h} \lambda(x) d x \exp\left(\int_{t}^{t+h} \lambda(x) d x \right)=\lambda(t) h+o(h)$.
4. $\mathrm{P}\left\{N(t, t+h]>1\right\}=\sum_{k=2}^{\infty} \frac{\left(\int_{t}^{t+h} \lambda(x) d x\right)^{k}}{k !} \exp\left(\int_{t}^{t+h} \lambda(x) d x \right)=o(h)$.

And we get the inhomogeneous Poisson process.

**Definition 2**. A counting measure $\nu$ on a space $\mathcal{S}$ is a measure with the following additional properties:

1. $\nu(A)$ is non-negative-integer valued for any measurable $A$.
2. $\nu(A)<\infty$ for any bounded, measurable $A$.

Any counting measure $\nu$ on $\mathcal{S}$ has the form

$$
\nu=\sum_{i} k_{i} \delta_{x_{i}}
$$

for a countable collection of positive integers $k_{i}$ and points $x_{i} \in \mathcal{S}$, where $\delta_{x}$ is a point-mass at $x$. If all the $k_{i}=1$, then $\nu$ is said to be a simple counting measure.

**Definition 3**. Let $(\mathcal{X}, \mathcal{B})$ be some measurable space and let $(\Omega, \mathcal{F}, \mathrm{E})$ be a probability space. A random measure is a mapping $M: \Omega \times \mathcal{B} \rightarrow \mathbb{R}$ such that

1. For each $\omega \in \Omega, M(\omega, \cdot)$ is a measure on $(\mathcal{X}, \mathcal{B})$.
2. For each $A \in \mathcal{B}, M(\cdot, A)$ is a real-valued random variable.

We can also think of a random measure as a stochastic process indexed by $\mathcal{B}$.

If the measure $M(\omega, \cdot)$ has some property - finiteness, $\sigma$-finiteness, integer-valued, and so forth - for all (almost all) $\omega \in \Omega$, then we say that the random measure has that property as well (almost surely).

Note: As with random variables generally, we suppress the $\omega$ argument in common usage. Thus, the random measure of a set $A$ is written as the random variable $M(A)$.

**Definition 4**. If $M$ is a random measure on $(\mathcal{X}, \mathcal{B})$, then $m(A)=\mathrm{E} M(A)$ is a measure on $(\mathcal{X}, \mathcal{B})$ called the mean measure.

**Definition 5**. A point process is a random counting measure.

If the random counting measure is simple, we say that the process is a simple point process.

**Example 6**. Let $\left(X_{n}\right)_{n \geq 0}$ be a collection of random variables taking values in the same measurable space $(\mathcal{X}, \mathcal{B})$. Define

$$
M(\omega, A)=\sum_{n \geq 0} 1_{A}\left(X_{n}(\omega)\right)
$$

Then, for each $\omega$, we get a counting measure because it satisfies equation (2). For each $A \in \mathcal{B}$, we get a random variable because summation and composition with $`1_{A}`$ are measurable. The point process is simple as long as the $`X_{n}`$ s are distinct with probability 1.

Speaking loosely, we can invert this process. Given a point process $M$, we can find the "points" of the counting measure for each $\omega$ and define a sequence of random variables such that equation (3) holds. This can be made rigorous under some conditions.

**Example 7**. The homogeneous and inhomogeneous Poisson processes.

See Perspective 1 above.

**Example 8**. Let $M$ be a non-negative integer-valued random variable with distribution $G$. Given $M=m$, draw $X_{1}, \ldots, X_{m}$ be IID from $F$. Define

$$
N(A)=\sum_{i=1}^{M} 1\left\{X_{i} \in A\right\} .
$$

Then, $N(A)$ is a point process, a special case of the above.

**Example 9**. Poisson Random Measure (aka Poisson Scatter)

Let $N$ be a point process with mean measure $\nu$ satisfying the following:

1. For each $A \in \mathcal{B}, N(A)$ has a Poisson $\langle\nu(A)\rangle$ distribution.
2. Whenever $A_{1}, \ldots, A_{n} \in \mathcal{B}$ for $n \geq 2$ are disjoint, $N\left(A_{1}\right), \ldots, N\left(A_{n}\right)$ are independent random variables.

**Question 10**. Do we have to check for a conflict between the two conditions for a Poisson random measure? What conflict?

**Question 11**. Let $f$ be a probability density on $\mathbb{R}^{2}$. Generate a random scatter of points as follows:

1. Let $N_{\text {total }}$ be a Poisson $\langle\lambda\rangle$ number with $\lambda>0$.
2. Given $N_{\text {total }}=n$, draw $n$ points IID from $f$.

Is this a Point Process? Describe it.

Solution: Denote $p:=\int_Af(x)dx$. For each $A\in\mathcal{B}(\mathbb{R}^2)$, we have

$$
\begin{aligned}
\mathrm{P}(N(A)=n)  &=\sum_{m=n}^{\infty}\mathrm{P}(N_{\text{total}}=m)\binom{m}{n}p^n(1-p)^{m-n}\\
 &=\sum_{m=n}^{\infty}\frac{\lambda^me^{-\lambda}}{m!}\binom{m}{n}p^n(1-p)^{m-n} \\
&=\frac{(\lambda p)^ne^{-\lambda}}{n!}\sum_{m=n}^{\infty}\frac{(\lambda(1-p))^{m-n}}{(m-n)!}\\
&=\frac{(\lambda p)^ne^{-\lambda}}{n!}e^{\lambda(1-p)}\\
&=\frac{(\lambda p)^ne^{-\lambda p}}{n!}
\end{aligned}
$$

which is the probability density function of Poisson distribution.

**Question 12**. Suppose that Pittsburgh city buses arrive a bus stop every $\Delta>0$ minutes and that a passanger arrives at the bus stop at some random point uniformly distributed over the day. How long on average does the passenger wait?

Solution: the expectation of $\textrm{Uniform}(0,\Delta)$ is $\frac{\Delta}{2}$

Suppose instead that the arrival times of the buses at the stop are described by a Poisson random measure on the line with mean measure $\nu=\mathrm{Leb} / \Delta$. How long on average does the passenger wait?

Solution: by the memoryless property of Poisson process, the average time that passenger wait is the expectation of $\mathrm{Exp}\left(\frac{1}{\Delta}\right)$, which is $\Delta$.

What's going on here?

**Question 13**. Suppose the positions of trees in a forest are well characterized by a Poisson random measure $N$ with mean measure $\nu=\lambda$ Leb for $\lambda>0$. What is the distribution of the distance from an arbitrary point in the forest to the nearest tree? What is the distance from that point that you can see to the east?

Solution: if $R$ measures the distance to nearest neighbor $n_x$ of some point $x$, then

$$
\mathrm{P}(R>r) = \mathrm{P}(\text{no points in a ball of radius }r\text{ around }x)  = e^{-\lambda\pi r^2},
$$

So $R$ has the Rayleigh distribution with parameter $\sigma = 1/(\lambda\pi\sqrt{2})$.

**Example 14**. Let $M$ be a non-negative integer-valued random variable with distribution $G$. Let $\mu$ be a probability measure on a space $\mathcal{S}$.

Given any $k-1$ disjoint sets $A_1, \ldots, A_{k-1}$, let $A_k=\left(\bigcup_{j=1}^{k-1} A_j\right)^c$. Let

$$
\mathrm{P}\left\{N\left(A_{j}\right)=n_{j}, j=1, \ldots, k|M\right\}=\left(\begin{array}{c}
M \\
n_{1}, \ldots, n_{k}
\end{array}\right) \mu^{n_{1}}\left(A_{1}\right) \cdots \mu^{n_{k}}\left(A_{k}\right) .
$$

What is the marginal distribution of $N\left(A_{1}\right)$ here? Note that $M=N(\mathcal{S})$.

We care not just about the distributions of $N(A)$ but also moments. For example, if $A_{1}$ and $A_{2}$ partition $\mathcal{S}$ :

$$
\mathrm{E}\left(N\left(A_{1}\right) N\left(A_{2}\right) \mid M\right)=M(M-1) \mu\left(A_{1}\right) \mu\left(A_{2}\right) .
$$

*proof*:

$$
\begin{aligned}
&\mathrm{E}(N(A)_1N(A_2)|M)\\
&=\sum_{n_1=0}^Mn_1(M-n_1)\binom{M}{n_1}\mu^{n_1}(A_1)(1-\mu(A_1))^{M-n_1}\\
&=M(M-1)\mu(A_1)(1-\mu(A_1))\sum_{n_1=1}^{M-1}\binom{M-2}{n_1-1}\mu^{n_1-1}(A_1)(1-\mu(A_1))^{M-n_1-1}\\
&=M(M-1)\mu(A_1)(1-\mu(A_1))\ \ \ \ \square
\end{aligned}
$$

So $\operatorname{Cov}\left(N\left(A_{1}\right), N\left(A_{2}\right)\right)=c_{[2]} \mu\left(A_{1}\right) \mu\left(A_{2}\right)$, where $c_{[2]}$ is second factorial cumulant of $M$.

These distributions and moments are easiest to study with generating functions.

**Practical Motivation 15**. There are two threads to keep hold of when delving into the theory of point processes. The first is that we want to be able to characterize the implications of our assumptions for the structure of the point scatter. The abstract tools are designed to capture different features of this structure to help us understand it better. The second thread is that in practice we are faced with point scatters with different types of structure, and we want to be able to generate models that match what we observe.

The following properties, which will be more fully developed in homework, give insight into different features of the process.

**Stationarity 16**. A point process $N$ is stationary if for all bounded, measurable sets $A_{1}, \ldots, A_{k}$ for $k \geq 1$, the joint distribution of $N\left(A_{1}+t\right), \ldots, N\left(A_{k}+t\right)$ does not depend on $t$ over all $t$ such that the shifted sets $A_{i}+t$ remain in the domain of $N$.

**Question 17**. Given an example of a stationary point process.

Solution: See Q11 and let $f$ be a uniform distribution on the unit square $[0,1]\times[0,1]$.

**Definition 18**. Covariance Measure.

Let $N$ be a point process with mean measure $\nu$, and define

$$
C_{2}(A, B)=\operatorname{Cov}(N(A), N(B))
$$

What does this mean? Can we simplify this in the stationary case?

**Definition 19**. Avoidance Measure.

Let $N$ be a point process. Write

$$
V(A)=\mathrm{P}\left\{N(A)=0\right\} .
$$

What is the avoidance measure for the Poisson random measure?

**Definition 20**. The $K$ function.

The $K$-function $K(t)$ for a stationary point process is defined as the expected number of points within a distance $t$ of a typical point, divided by the overall intensity of the process.

For a homogeneous Poisson scatter in $d$ dimensions, $K(t)=v_{d} t^{d}$, where $v_{d}$ is the volume of the unit ball in $d$ dimensions.

**Idea 21**. Integrating Random Measures

We have seen that with a measure, one can compute the measure of sets or integrate functions and that the latter is a more general representation.

If $M$ is a random measure, then $\int f d M$ is a random variable which gives for each $\omega \in \Omega$, $\int f d M(\omega, \cdot)$.

It is often useful to represent random measures via their integrals.

**Definition 22**. Laplace Functionals If $M$ is a random measure, then

$$
L[f]=\mathrm{E} e^{-\int f d M},
$$

for non-negative, measurable functions $f$, is called the Laplace functional of the process.

The Laplace functional determines the distribution of the process.

**Question 23**. What is the Laplace functional of the Poisson random measure?

**Definition 24**. Probability Generating Functional

Let $N$ be a point process and define

$$
G_{N}[h]=\mathrm{E} e^{\int \log h d N}=\mathrm{E} \prod_{i} h\left(X_{i}\right),
$$

where $0 \leq h \leq 1$ is a function with $1-h$ vanishing outside a compact set and where the latter product is over the "points" of the process.

This also determines the distribution of the process.

**Reminder 25**. What does "distribution of the process" of the process mean?
